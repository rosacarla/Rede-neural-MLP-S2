# -*- coding: utf-8 -*-
"""Rede-Neural-MLP-S2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/172BHC3NykNnVHgr57ISfMRmdwSo0sWLR

# **TESTE DE REDE NEURAL MLP (MULTI-LAYER PERCEPTRON)**
CURSO: INTELIG√äNCIA ARTIFICIAL APLICADA  
DISCIPLINA: REDES NEURAIS  
PROFESSOR: Edson Ruschel (autor do c√≥digo original MLP_S2.py)  
OBJETIVO: a rede deve identificar n√∫meros escritos (ou desenhados) manualmente por seres humanos em tarefa de vis√£o computacional  
ESTUDANTE: Carla Edila Silveira  
DATA: 08/09/2023

<img src='https://machinelearninggeek.com/wp-content/uploads/2021/03/Multi-Layer-Perceptron-Neural-Network-using-Python-800x445.jpg' width=80% height=50%>
"""

# IMPORTA√á√ÉO DE BIBLIOTECAS
import matplotlib.pyplot as plt  # Necessario instalar matplotlib (Spyder, pip)
import numpy as np
import tensorflow as tf  # Necessario atualizar devido conflito em metricas
from keras.src.layers import Dense
from sklearn.model_selection import train_test_split
import tensorflow._api.v2 as tf  # Necessario para para ajustar o compat
import tensorflow.compat.v2 as tf # Necessario por erro de execu√ß√£o no Spyder
from tensorflow.keras import Sequential

# CRIA√á√ÉO DO MODELO MLP

model = Sequential()

# DEFINI√á√ÉO DA QUANTIDADE DE NEUR√îNIOS DAS CAMADAS

# Quantidade de neur√¥nios da Camada Oculta 1
n1 = 10

# Quantidade de neur√¥nios da Camada de Sa√≠da
ns = 10

# DEFINI√á√ÉO DAS FUN√á√ïES DE ATIVA√á√ÉO
    # relu    -> Rectified Linear Unit (Unidade Linear Retificada)
    # sigmoid -> sigmoid(x) = 1 / (1 + exp(-x))
    # tanh    -> Tangente hiperb√≥lica
    # softmax -> Utilizada na camada de sa√≠da

# Fun√ß√£o de Ativa√ß√£o da Camada Oculta 1
fa1 = 'tanh'

# Fun√ß√£o de Ativa√ß√£o da Camada de Sa√≠da
fas = 'softmax'

# ADI√á√ÉO DE CAMADAS √Ä REDE NEURAL

# Primeira Camada Oculta (o input_dim √© a pr√≥pria camada de entrada)
model.add(Dense(units=n1, activation=fa1, input_dim=784))

# Camada de Sa√≠da
model.add(Dense(units=ns, activation=fas))

# DEFINI√á√ÉO DA FUN√á√ÉO DE PERDA
    # mean_squared_error       -> Erro quadr√°tico m√©dio.
    # binary_crossentropy      -> Entropia cruzada bin√°ria.
    # categorical_crossentropy -> Entropia cruzada categ√≥rica.
    # mean_absolute_error      -> Erro absoluto m√©dio.

fp = 'mean_absolute_error'

# DEFINI√á√ÉO DO OTIMIZADOR
    # sgd     -> Descida de gradiente estoc√°stico (SGD).
    # adam    -> SGD com adapta√ß√£o de taxa de aprendizado.
    # rmsprop -> Baseado em Root Mean Square Propagation.
    # adagrad -> Adapta a taxa de aprendizado para cada par√¢metro.

otimizador = 'adam'

"""### üß† DEFINI√á√ÉO DAS M√âTRICAS
    # accuracy            -> Acur√°cia.
    # precision           -> Precis√£o.
    # recall              -> Revoca√ß√£o.
    # f1-score            -> Pontua√ß√£o F1.
    # mean_squared_error  -> Erro quadr√°tico m√©dio.
    # mean_absolute_error -> Erro absoluto m√©dio.


"""

# custom_metric = 'accuracy'

"""‚ùó<b>OBSERVA√á√ÉO:</b> Essa m√©trica foi renomeada para tratar conflito entre nomes de m√©tricas de acur√°cia padr√£o Keras;
como n√£o resolveu o erro, ent√£o a solu√ß√£o foi retir√°-la do c√≥digo.
"""

# COMPILA√á√ÉO DO MODELO

model.compile(loss=fp, optimizer=otimizador, metrics=['accuracy'])
# adicionada metrica de acuracia do padrao Keras para eliminar conflito
# model.compile(loss=fp, optimizer=otimizador, metrics=[custom_metric])

# CARREGAMENTO DOS DADOS DE TREINAMENTO E TESTE

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# PR√â-PROCESSAMENTO DOS DADOS

# Redimensionar as imagens para um vetor unidimensional
x_train = x_train.reshape(-1, 784)
x_test = x_test.reshape(-1, 784)

# Converter para tipo float32
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalizar os valores dos pixels para o intervalo [0, 1]
x_train /= 255.0
x_test /= 255.0

# TRANSFORMA√á√ÉO DOS R√ìTULOS EM CODIFICA√á√ÉO ONE-HOT

y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# DEFINI√á√ÉO DO PERCENTUAL DO CONJUNTO DE DADOS DE TESTE

dados_teste = 0.90 # 0.90 significa 90% para teste e 10% para treinamento

# DIVIS√ÉO DOS DADOS EM CONJUNTOS DE TREINAMENTO E TESTE

x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=dados_teste, random_state=42)

# DEFINI√á√ÉO DO N√öMERO DE √âPOCAS E N√öMERO DE AMOSTRAS

epocas = 2
amostras = 256

# TREINAMENTO DA REDE NEURAL MLP

model.fit(x_train, y_train, epochs = epocas, batch_size = amostras)

# AVALIA√á√ÉO DO MODELO (PERDA E ACUR√ÅCIA)

loss, accuracy = model.evaluate(x_test, y_test)

# REALIZA√á√ÉO DE PREVIS√ïES COM DADOS ORIGINAIS

predictions = model.predict(x_test)

# SELE√á√ÉO DO N√öMERO DE AMOSTRAS PARA TESTE

# Selecione valores maiores que zero para testar
amostras_previsao = 20

# Sorteio de amostras de acordo com o n√∫mero de amostras (amostras_previsao)
samples = np.random.choice(len(x_test), amostras_previsao)
idx=0

# EXIBI√á√ÉO DAS IMAGENS E PREVIS√ïES CORRESPONDENTES

# N√∫mero de colunas que deseja exibir
num_colunas = 2

# N√∫mero de amostras que deseja exibir
amostras_previsao = 20

# Sorteie amostras de acordo com o n√∫mero de amostras (amostras_previsao)
samples = np.random.choice(len(x_test), amostras_previsao)

# Tamanho das figuras (6 cm de altura e 3 cm de largura)
figsize = (5, 4)

# Espa√ßamento entre as colunas
hspace = 0.1

# Crie uma figura com subplots
num_linhas = (amostras_previsao + num_colunas - 1) // num_colunas
fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(figsize[0] * num_colunas, figsize[1] * num_linhas))

for i, idx in enumerate(samples):
    image = x_test[idx].reshape((28, 28))  # Redimensionar a imagem para o formato original
    label_true = np.argmax(y_test[idx])  # R√≥tulo do n√∫mero real
    label_pred = np.argmax(predictions[idx])  # R√≥tulo do n√∫mero previsto

    # Exiba a imagem na subfigura correspondente
    ax = axs[i // num_colunas, i % num_colunas]
    ax.imshow(image, cmap='gray')
    ax.set_title(f'Real: {label_true}\nReconhecido: {label_pred}')
    ax.axis('off')

    # Ajuste o espa√ßamento entre as colunas
    plt.subplots_adjust(wspace=hspace, hspace=0.5)

# Salvar a figura em um arquivo (por exemplo, PNG)
plt.savefig('imagens_previsoes.png')

# Exiba a figura
plt.show()

# Exibir a Perda e a Precis√£o
print(f"Fun√ß√£o de Perda: {loss:.4f}")
print(f"Precis√£o: {accuracy:.4f}")

"""### üéØM√âTRICAS POR EXECU√á√ÉO
** Camada com 2 neur√¥nios  
1a.) Fun√ß√£o de Perda: 0.1724 | Precis√£o: 0.2235  
2a.) Fun√ß√£o de Perda: 0.1749 | Precis√£o: 0.2270  
3a.) Fun√ß√£o de Perda: 0.1742 | Precis√£o: 0.2690  
4a.) Fun√ß√£o de perda: 0.1721 | Precisao: 0.2691  
** Camada com 3 neur√¥nios    
5a.) Fun√ß√£o de perda: 0.1700 | Precisao: 0.3021  
6a.) Fun√ß√£o de Perda: 0.1691 | Precis√£o: 0.4550  
7a.) Fun√ß√£o de Perda: 0.1712 | Precis√£o: 0.2496  
8a.) Fun√ß√£o de Perda: 0.1689 | Precis√£o: 0.3157  
** Camada com 4 neur√¥nios  
9a.) Fun√ß√£o de Perda: 0.1648 | Precis√£o: 0.4525  
10a.) Fun√ß√£o de Perda: 0.1616 | Precis√£o: 0.4532  
11a.) Fun√ß√£o de Perda: 0.1664 | Precis√£o: 0.4347  
12a.) Fun√ß√£o de Perda: 0.1643 | Precis√£o: 0.4308  
** Camada com 5 neur√¥nios  
13a.) Fun√ß√£o de Perda: 0.1602 | Precis√£o: 0.4765  
** Camada com 6 neur√¥nios  
14a.) Fun√ß√£o de Perda: 0.1576 | Precis√£o: 0.5043  
** Camada com 7 neur√¥nios  
15a.) Fun√ß√£o de Perda: 0.1554 | Precis√£o: 0.5028  
** Camada com 8 neur√¥nios  
16a.) Fun√ß√£o de Perda: 0.1451 | Precis√£o: 0.5569  
** Camada com 9 neur√¥nios  
17a.) Fun√ß√£o de Perda: 0.1389 | Precis√£o: 0.5787  
** Camada com 10 neur√¥nios  
18a.) Fun√ß√£o de Perda: 0.1399 | Precis√£o: 0.6443

## ‚öõ <b>AN√ÅLISE DO MODELO</b>

<p align="justify">FINALIDADE: fornecer elementos b√°sicos para entender o funcionamento de uma Rede Neural MLP; possibilitar a altera√ß√£o de par√¢metros do c√≥digo em linguagem Python para compreender as mudan√ßas de desempenho do modelo durante e ap√≥s o treinamento da rede.</p>

### QUEST√ÉO 1
<p align="justify">a) Rodar o modelo com o c√≥digo em seu formato original. Provavelmente, alguns n√∫meros entre os 20 testados (e visualizados) n√£o ser√£o identificados de forma correta nos resultados.</p>
<p align="justify">b) Qual foi o valor obtido para a ‚Äúprecis√£o‚Äù do modelo? Justifique, com suas palavras, o porqu√™ de a precis√£o ser t√£o baixa com essas configura√ß√µes originais.</p>  
<p align="justify">c) Quais s√£o os par√¢metros configur√°veis que mais est√£o impactando na precis√£o do modelo e por qu√™?</p>

### RESPOSTAS
A precis√£o (<i>accuracy</i>) baixa do modelo pode ter ocorrido por diversas raz√µes com base nas configura√ß√µes originais do modelo:

<p align="justify">1. <b>N√∫mero de Neur√¥nios</b>: A camada oculta tem apenas 2 neur√¥nios. Redes neurais rasas com poucos neur√¥nios podem n√£o ser capazes de capturar caracter√≠sticas complexas dos dados. Aumentar o n√∫mero de neur√¥nios na camada oculta pode melhorar a capacidade do modelo.</p>

<p align="justify">2. <b>Fun√ß√£o de Ativa√ß√£o</b>: √â uma escolha v√°lida, mas outras fun√ß√µes como 'relu' geralmente funcionam bem para redes neurais profundas. Experimentar fun√ß√µes de ativa√ß√£o diferentes pode ter um impacto na precis√£o.</p>

<p align="justify">3. <b>Fun√ß√£o de Perda</b>: 'mean_absolute_error' pode n√£o ser apropriada para um problema de classifica√ß√£o multiclasse como o MNIST. Geralmente, a 'categorical_crossentropy' √© usada para problemas de classifica√ß√£o.</p>

<p align="justify">4. <b>Otimizador</b>: 'adam' √© uma escolha razo√°vel, mas diferentes otimizadores podem funcionar melhor de acordo com o problema. O ajuste dos hiperpar√¢metros do otimizador, como a taxa de aprendizado, tamb√©m pode ser necess√°rio.</p>

<p align="justify">5. <b>N√∫mero de √âpocas e Tamanho do Lote (Batch Size)</b>: Com apenas 2 √©pocas de treinamento e um tamanho de lote pequeno (256 amostras), o modelo pode n√£o ter convergido completamente. Aumentar o n√∫mero de √©pocas e o tamanho do lote pode ajudar a melhorar a precis√£o.</p>

<p align="justify">6. <b>Arquitetura da Rede</b>: Uma rede neural MLP simples pode n√£o ser suficiente para alcan√ßar alta precis√£o no MNIST. Arquiteturas mais complexas, como redes neurais convolucionais (CNNs), s√£o mais adequadas para tarefas de vis√£o computacional.</p>

<p align="justify">7. <b>Pr√©-processamento de Dados</b>: O pr√©-processamento dos dados √© muito importante. Recomenda-se confrimar que os dados estejam normalizados corretamente e as codifica√ß√µes one-hot para r√≥tulos estejam corretas.</p>

<p align="justify">8. <b>Divis√£o de Dados</b>: A divis√£o entre conjuntos de treinamento e teste pode afetar a precis√£o. Recomenda-se confirmar que a divis√£o foi feita de modo aleat√≥rio e representativo.</p>

<p align="justify">9. <b>Tamanho da Camada de Sa√≠da</b>: A camada de sa√≠da tem 10 unidades, o que √© apropriado para o MNIST, mas pode ser necess√°rio ajustar para outros conjuntos de dados.</p>

<p align="justify">10. <b>Inicializa√ß√£o de Pesos</b>: A inicializa√ß√£o de pesos padr√£o pode afetar o treinamento. Experimentar inicializa√ß√µes de pesos diferentes, como Xavier ou He, pode ser ben√©fico.</p>

<p align="justify"><b>PARA MELHORAR A PRECIS√ÉO:</b> √© poss√≠vel come√ßar com ajuste do n√∫mero de neur√¥nios na camada oculta, experimentar fun√ß√µes de ativa√ß√£o diferentes, trocar fun√ß√£o de perda para 'categorical_crossentropy', ajustar a taxa de aprendizado e aumentar o n√∫mero de √©pocas de treinamento. Al√©m disso, considerar uso de arquiteturas mais avan√ßadas, como CNNs, pode melhorar ainda mais o desempenho.</p>

### QUEST√ÉO 2
<p align="justify">a) De acordo com sua resposta na quest√£o 1, alterar os par√¢metros identificados como impactantes na precis√£o do modelo e rodar o treinamento e teste novamente.</p>
<p align="justify">b) Quais os novos valores utilizados para os par√¢metros configur√°veis que foram alterados e por que foram escolhidos tais valores?</p>
<p align="justify">c) Qual foi o valor obtido para a ‚Äúprecis√£o‚Äù desta vez?</p>

### RESPOSTAS
Foi utilizado o valor 3 para o par√¢metro da camada oculta de neur√¥nios, para observar se haveria melhoria das m√©tricas (taxa de perda e precis√£o). Na 1a execu√ß√£o com 3 neur√¥nios, foi obtido um resultado melhor do que nas execu√ß√µes anteriores:   
- <b>Fun√ß√£o de Perda: 0.1700 | Precis√£o: 0.3021</b>  

Considerando o total de 5 execu√ß√µes, a fun√ß√£o de perda foi a menor e a precis√£o foi a maior. Houve 6 identifica√ß√µes corretas entre 20 tentativas, conforme as imagens reproduzidas para visualiza√ß√£o.

### QUEST√ÉO 3
<p align="justify">a) √â poss√≠vel realizar melhorias na estrutura do c√≥digo (arquitetura da rede) para conseguir desempenho ainda melhor do modelo?</p>
<p align="justify">b) Se sim, cite alguns exemplos de altera√ß√£o para que tal melhoria de desempenho seja alcan√ßada.</p>

### RESPOSTAS
<p align="justify">√â poss√≠vel acrescentar melhorias na arquitetura da rede neural para obter desempenho melhor do modelo, desde que tais melhorias estejam dentro das restri√ß√µes especificadas de n√£o introduzir novos componentes ou t√©cnicas na rede neural. Alguns exemplos de altera√ß√µes que podem aprimorar o desempenho do modelo seriam estes:</p>

<p align="justify">1.<b>Aumentar o n√∫mero de neur√¥nios na camada oculta:</b> O aumento pode permitir que o modelo aprenda caracter√≠sticas mais complexas dos dados, mas √© preciso cuidado para n√£o aumentar demais, pois isso pode levar a <i>overfitting</i> (treino excessivo).</p>

<p align="justify">2.<b>Adicionar camadas ocultas:</b> O acrescimo de camadas ocultas √† rede neural para torn√°-la mais profunda. Redes mais profundas t√™m o potencial de aprender representa√ß√µes mais abstratas dos dados.</p>

<p align="justify">3.<b>Ajustar a taxa de aprendizado:</b> Experimentar diferentes valores para a taxa de aprendizado (<i>learning rate</i>), pois uma taxa de aprendizado adequada √© fundamental para o treinamento bem-sucedido de uma rede.</p>

<p align="justify">4.<b>Aumentar o n√∫mero de √©pocas:</b>  H√° possibilidade de  treinar a rede por mais √©pocas para permitir que continue aprendendo. No entanto, √© necess√°rio observar o risco de <i>overfitting</i></p>

<p align="justify">5. <b>Experimentar diferentes fun√ß√µes de ativa√ß√£o:</b> Testar diferentes fun√ß√µes de ativa√ß√£o nas camadas ocultas, como ReLU, Leaky ReLU ou ELU, para ver qual funciona melhor para o problema.</p>

<p align="justify">6. <b>Ajustar o tamanho do lote (batch size):</b> Alterar o tamanho do lote pode afetar o processo de treinamento. Experimentar diferentes tamanhos de lote possibilita ver qual funciona melhor.</p>

<p align="justify">7. <b>Usar regulariza√ß√£o: </b>Se necess√°rio, adicionar regulariza√ß√£o L1 ou L2 √†s camadas para evitar <i>overfitting</i>.</p>

<p align="justify">8. <b>Experimentar otimizadores:</b> Al√©m do Adam, h√° outros otimizadores, como o RMSprop ou SGD com momento, para testar e ver se melhoram o desempenho.</p>

<p align="justify">9. <b>Tamanho da camada de sa√≠da:</b> Ajustar o n√∫mero de neur√¥nios na camada de sa√≠da, dependendo do problema de classifica√ß√£o. √â importante se certificar de que corresponda ao n√∫mero de classes no conjunto de dados.</p>

<p align="justify"><b>CONSIDERA√á√ïES FINAIS:</b> Essas s√£o apenas algumas sugest√µes. A escolha dos hiperpar√¢metros depende do conjunto de dados espec√≠fico e do problema que h√° para resolver. √â importante realizar experimentos e ajustes iterativos para encontrar a configura√ß√£o que oferece o melhor desempenho para cada caso. Por fim, √© fundamental monitorar o desempenho em um conjunto de valida√ß√£o para evitar <i>overfitting</i>.</p>
"""